+++
title = 'Optimising Cloud Storage in AWS and finding the secret cost report'
date = 2025-01-06T15:29:43+11:00
draft = true
tags = ['AWS', 'CIO', 'Cloud', 'SaaS', 'S3']
revision = 1
+++

![Visibility and Transparency](https://toobstar.github.io/images/caveman_light.jpg)

# Cloud Storage using AWS

Most businesses operating in AWS will typically end up being heavy users of their storage (infrastructure as a) service (IaaS) S3.  The key parameters when considering a cloud storage setup are:

- Durability and availability
The ability to easily control redundancy and performance capabilities via configuration is extremely powerful.  

- Security and compliance
With cloud storage you can control where your data is stored, who can access it, whether it can be modified, and if and how it is encrypted.  

## Models of Use

The most common uses of storage are related to:

- Application Management
- Datalakes 
- Backup and Disaster Recovery (DR)
- Audit, Compliance and Archive

Each of these will have different parameters that will be optimised each use case.

## Operational Challenges

The challenge for the infrastructure manager is that data is easy to create or receive, but much harder to delete. Past business orthodoxy is that *data was the new oil*, and that holding data would enable the success of past oil barons.  This belief in the opportunity enabled by data, and the hesitance faced by anyone asked if a specific item can be removed *permanently* has resulted in ever increasing data foot prints. But more recent thinking in the light of many serious and highly damaging data breaches is [that data is more like uranium than oil](https://www.google.com/search?q=data+is+the+new+uranium).   Note: I believe it's mandatory at every conference (no matter what the topic is) for someone to mention this.  The metaphor being that data while powerful and with the ability to enable a business to succeed, can also be dangerous if mishandled or allowed to escape from control. 

In theory building a system that can self-manage through automated retention rules can avoid a lot of this pain. In practice (at least where humans are involved) it can be hard to achieve.  This ever-expanding data footprint will likely proceed unnoticed until the cost of it forces there to be attention applied.  It's when you start trying to manage [the cost of S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/aws-usage-report-understand.html) that you really start diving into the configuration options that are available. 
 
## Managing Usage and Costs

The cost explorer will give the basics quite easily and allow you to present cost and filter/or group by all the usual parameters you would want.

![AWS S3 Cost Report](https://toobstar.github.io/images/aws_s3_cost.jpg)

The issue soon arises that cost per GB varies depending on how you have your S3 configured.  And actually further complicating that picture is that file count matters and many smaller files under certain circumstances can end up costing much more than fewer larger files of the same overall size. 

## Activating the secret Storage usage report

The cost explorer claims to present cost *and usage* but by default it actually only shows cost. To access the usage panel you need to do a not-obvious filtering step of navigating to the **Usage type** filtering menu.  From there you need to search for "timedstorage" and then enable all options (there were 28 when I last ran this but presumably that will change over time and will need be reviewed). 

![AWS Cost Report Timed Storage setting](https://toobstar.github.io/images/aws_cost_timedstorage.jpg)

After you do this and without any obvious reason why, a new panel will accompany the cost one which is *actually usage*.


![AWS Cost Report Usage](https://toobstar.github.io/images/aws_s3_usage)




FA Frequent Access
IA Infrequent Access
AA Archive Access
AIA Archive Instant Access
DAA Deep Archive Access




https://aws.amazon.com/s3/storage-classes/intelligent-tiering/




