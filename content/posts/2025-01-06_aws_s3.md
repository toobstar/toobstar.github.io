+++
title = 'Optimising Cloud Storage in AWS and finding the secret cost report'
date = 2025-01-06T15:29:43+11:00
draft = true
tags = ['AWS', 'CIO', 'Cloud', 'SaaS', 'S3']
revision = 1
+++

![Visibility and Transparency](https://toobstar.github.io/images/caveman_light.jpg)

# Cloud Storage using AWS

Most businesses operating in AWS will typically end up being heavy users of their storage (infrastructure as a) service (IaaS) S3.  The key parameters when considering a cloud storage setup are:

- Durability and availability
The ability to easily control redundancy and performance capabilities via configuration is extremely powerful.  

- Security and compliance
With cloud storage you can control where your data is stored, who can access it, whether it can be modified, and if and how it is encrypted.  

## Models of Use

The most common uses of storage are related to:

- Application Management
- Datalakes 
- Backup and Disaster Recovery (DR)
- Audit, Compliance and Archive

Each of these will have different parameters that will be optimised each use case.

## Operational Challenges

The challenge for the infrastructure manager is that data is easy to create or receive, but much harder to delete. Past business orthodoxy is that *data was the new oil*, and that holding data would enable the success of past oil barons.  This belief in the opportunity enabled by data, and the hesitance faced by anyone asked if a specific item can be removed *permanently* has resulted in ever increasing data foot prints. But more recent thinking in the light of many serious and highly damaging data breaches is [that data is more like uranium than oil](https://www.google.com/search?q=data+is+the+new+uranium).   Note: I believe it's mandatory at every conference (no matter what the topic is) for someone to mention this.  The metaphor being that data while powerful and with the ability to enable a business to succeed, can also be dangerous if mishandled or allowed to escape from control. 

In theory building a system that can self-manage through automated retention rules can avoid a lot of this pain. In practice (at least where humans are involved) it can be hard to achieve.  This ever-expanding data footprint will likely proceed unnoticed until the cost of it forces there to be attention applied.  It's when you start trying to manage [the cost of S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/aws-usage-report-understand.html) that you really start diving into the configuration options that are available. 
 
## Managing Usage and Costs

The cost explorer will give the basics quite easily and allow you to present cost and filter/or group by all the usual parameters you would want.

FA Frequent Access
IA Infrequent Access
AA Archive Access
AIA Archive Instant Access
DAA Deep Archive Access




https://aws.amazon.com/s3/storage-classes/intelligent-tiering/




Cost Explorer -> add filter "TimedStorage" then "magic"