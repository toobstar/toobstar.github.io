+++
title = 'Emerging patterns in GenAI Risk Control'
date = 2025-04-02T13:04:53+11:00
draft = true
tags = ['Risk', 'CIO', 'AI', 'GenAI']
revision = 1
+++


![Improving Gemini AI](https://toobstar.github.io/images/caveman_tree.jpg)

## Generative AI: Slow, then fast, then *really* fast

Without providing a deep overview of AI the history and use of it is surprising long.  With early research and use happening alongside the establishment of computer science even from the 1950s and the likes of Alan Turing. The work back then focussed on rule-based systems and provided basic probabilistic models that could generate simple text.

Through the 1990s and early 2000s neural networks and deep learning approaches developed to provide more practical commercial applications, but it will still at the fringe of engineering practice.  It has been in the last 10 years that the tranformer revolution and Generative Adversarial Networks (GANs) enabled AI to create realistic images and videos that we saw the use of AI become more central to engineering R&D efforts.  The demonstrations of capabilities were so impressive they could not be ignored. 

And since 2020 with the widespread release of large scale language models (LLMs) based on the transformer architecture with the likes of GPT, DALL-E and Stable Diffusion we have seen the use of these become mainstream and the attention for all forward looking businesses to be focussed on GenAI as a potential approach to rapidly improve product and service outcomes. 

## Risk Overview and Trust Frameworks

There are many obvious and well discussed potential issues that jump to mind with GenAI that include:

- data loss through the use of chat-interactions in an uncontrolled context
- the output containing hallucinations or factual errors that are undetected 
- privacy and copyright concerns with source data 
- bias and discrimination that reflect and reinforce the views of problematic source material

Following widespread discussions that have occured on these topics and the direct personal experience that many have had with these issues many companies were early to establish trust policies that attempted to address these concerns.  [Atlassian is an example](https://www.atlassian.com/trust/responsible-tech-principles) of one such organisation that was proactive here.  Governments around the world have followed including here in Australia with the [8 AI Ethics Principles published](https://www.industry.gov.au/publications/australias-artificial-intelligence-ethics-principles/australias-ai-ethics-principles), somewhat mirroring the widely deployed information-security framework [Essential 8](https://www.cyber.gov.au/resources-business-and-government/essential-cybersecurity/essential-eight).  

- **Human, societal and environmental wellbeing**: AI systems should benefit individuals, society and the environment.
- **Human-centred values**: AI systems should respect human rights, diversity, and the autonomy of individuals.
- **Fairness**: AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.
- **Privacy protection and security**: AI systems should respect and uphold privacy rights and data protection, and ensure the security of data.
- **Reliability and safety**: AI systems should reliably operate in accordance with their intended purpose.
- **Transparency and explainability**: There should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.
- **Contestability**: When an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.
- **Accountability**: People responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.

These principles are sensible and should be followed, but are not tactical in a way that a regular business can employ with ease.

## Governance and AI

As we move from principles based guidance to more practical governance and control frameworks I like the [approach presented by PWC](https://www.pwc.com/us/en/tech-effect/ai-analytics/managing-generative-ai-risks.html) as a starting point.  

![PWC AI Governance](https://toobstar.github.io/images/pwc_ai_governance.png)

It provides an overview of the key domains of governance and places them in context with each other:

- Strategy
- Control
- Responsible Practices
- Core Practices

The domain of risk control in AI have developed very quickly such that there is now an [ISO standard for AI Governance (38507)](https://www.iso.org/standard/56641.html) as well as a [NIST AI  Risk framework](https://www.nist.gov/itl/ai-risk-management-framework).  They are both (surprisingly) seemingly mature and well considered standards that would make sense to deployed in risk adverse organisations that have embraced AI.  

## Risk Management

Another practical and control focussed approach is to look at a vendor based view such as that presented by [Wiz](https://www.wiz.io/solutions/ai-spm).  At a [recent CISO conference](https://focusnetwork.co/cisoleaders.com.au/) Wiz presented an AI-pipeline with the various opportunities they have identified to apply risk controls.  

![Wiz AI Risk Control](https://toobstar.github.io/images/wiz-ai-risk-pipeline.png)

This is a very practical guide 







